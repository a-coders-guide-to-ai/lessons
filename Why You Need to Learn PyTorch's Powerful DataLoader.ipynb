{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Why You Need to Learn PyTorch's Powerful DataLoader.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "968MW8wNN10_"
      },
      "source": [
        "# Why You Need to Learn PyTorch's Powerful DataLoader\n",
        "-------------------------\n",
        "\n",
        "In case you missed it, in a previous article we went over [batch gradient descent](https://cutt.ly/cg2ai-batch-gradient-descent-medium) in-depth and saw how it vastly improved the [vanilla gradient descent](https://cutt.ly/cg2nn-ch1-medium) approach. In this article, we'll revisit batch gradient descent, but instead, we'll take advantage of PyTorch's powerful Dataset and DataLoader classes. By the end of this article, you will be convinced to never go back to a life of deep learning without PyTorch's DataLoader.\n",
        "\n",
        "Before we begin, we'll rerun through the steps which we performed in the previous [batch gradient descent article](https://cutt.ly/cg2ai-batch-gradient-descent-medium). Just like last time, we'll use the Pima Indians Diabetes dataset, set aside 33% for testing, standardize it and set the batch size to 64. We'll also keep the same neural network architecture - 1 layer of size 4.\n",
        "\n",
        "I've labeled the cell codes with very high level titles, but if you wish to see the in-depth explanation of the code below, please refer to the [previous article](https://cutt.ly/cg2ai-batch-gradient-descent-medium) where we introduced batch gradient descent.\n",
        "\n",
        "\n",
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "C5qF86LqN11N"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch.nn as nn"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ev1Qh4K63EY"
      },
      "source": [
        "### Import data and standardize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2P4akI_x7a8"
      },
      "source": [
        "df = pd.read_csv(r'https://raw.githubusercontent.com/a-coders-guide-to-ai/a-coders-guide-to-neural-networks/master/data/diabetes.csv')\r\n",
        "\r\n",
        "X = df[df.columns[:-1]]\r\n",
        "y = df['Outcome']\r\n",
        "X = X.values\r\n",
        "y = torch.tensor(y.values)\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\r\n",
        "\r\n",
        "scaler = StandardScaler()\r\n",
        "scaler.fit(X_train)\r\n",
        "X_train = torch.tensor(scaler.transform(X_train))\r\n",
        "X_test = torch.tensor(scaler.transform(X_test))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT-EhlH968Ox"
      },
      "source": [
        "### Create neural network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WfI8g9LN11q"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden_linear = nn.Linear(8, 4)\n",
        "        self.output_linear = nn.Linear(4, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, X):\n",
        "        hidden_output = self.sigmoid(self.hidden_linear(X))\n",
        "        output = self.sigmoid(self.output_linear(hidden_output))\n",
        "        return output"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlVGcCAc7E1C"
      },
      "source": [
        "### Make variables which will be reused"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoZhHj_eXpJt"
      },
      "source": [
        "def accuracy(y_pred, y):\r\n",
        "    return torch.sum((((y_pred>=0.5)+0).reshape(1,-1)==y)+0).item()/y.shape[0]\r\n",
        "\r\n",
        "epochs = 1000+1\r\n",
        "print_epoch = 100\r\n",
        "lr = 1e-2\r\n",
        "batch_size = 64"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDovWnAI7KCw"
      },
      "source": [
        "### Instantiate Model class and set loss and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ1v4uISaEsv"
      },
      "source": [
        "model = Model()\r\n",
        "BCE = nn.BCELoss()\r\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = lr)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D08oNi9U7WMt"
      },
      "source": [
        "### Run batch gradient descent without PyTorch's DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ml2-Rznwa52l",
        "outputId": "716c80f0-f377-4761-a593-1534481d1d6b"
      },
      "source": [
        "import numpy as np\r\n",
        "train_batches = int(np.ceil(len(X_train)/batch_size))-1\r\n",
        "test_batches = int(np.ceil(len(X_test)/batch_size))-1\r\n",
        "\r\n",
        "for epoch in range(epochs):\r\n",
        "    \r\n",
        "    iteration_loss = 0.\r\n",
        "    iteration_accuracy = 0.\r\n",
        "    \r\n",
        "    model.train()\r\n",
        "    for i in range(train_batches):\r\n",
        "      beg = i*batch_size\r\n",
        "      end = (i+1)*batch_size\r\n",
        "\r\n",
        "      y_pred = model(X_train[beg:end].float())\r\n",
        "      loss = BCE(y_pred, y_train[beg:end].reshape(-1,1).float())     \r\n",
        "      \r\n",
        "      iteration_loss += loss\r\n",
        "      iteration_accuracy += accuracy(y_pred, y_train[beg:end])\r\n",
        "\r\n",
        "      optimizer.zero_grad()\r\n",
        "      loss.backward()\r\n",
        "      optimizer.step()\r\n",
        "\r\n",
        "    if(epoch % print_epoch == 0):\r\n",
        "      print('Train: epoch: {0} - loss: {1:.5f}; acc: {2:.3f}'.format(epoch, iteration_loss/(i+1), iteration_accuracy/(i+1)))\r\n",
        "\r\n",
        "    iteration_loss = 0.\r\n",
        "    iteration_accuracy = 0.      \r\n",
        "\r\n",
        "    model.eval()\r\n",
        "    for i in range(test_batches):\r\n",
        "      beg = i*batch_size\r\n",
        "      end = (i+1)*batch_size\r\n",
        "      \r\n",
        "      y_pred = model(X_test[beg:end].float())\r\n",
        "      loss = BCE(y_pred, y_test[beg:end].reshape(-1,1).float())\r\n",
        "      \r\n",
        "      iteration_loss += loss\r\n",
        "      iteration_accuracy += accuracy(y_pred, y_test[beg:end])\r\n",
        "      \r\n",
        "    if(epoch % print_epoch == 0):\r\n",
        "        print('Test: epoch: {0} - loss: {1:.5f}; acc: {2:.3f}'.format(epoch, iteration_loss/(i+1), iteration_accuracy/(i+1)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: epoch: 0 - loss: 0.67015; acc: 0.646\n",
            "Test: epoch: 0 - loss: 0.67036; acc: 0.641\n",
            "Train: epoch: 100 - loss: 0.63613; acc: 0.646\n",
            "Test: epoch: 100 - loss: 0.64282; acc: 0.641\n",
            "Train: epoch: 200 - loss: 0.60189; acc: 0.646\n",
            "Test: epoch: 200 - loss: 0.61569; acc: 0.646\n",
            "Train: epoch: 300 - loss: 0.55953; acc: 0.705\n",
            "Test: epoch: 300 - loss: 0.58368; acc: 0.682\n",
            "Train: epoch: 400 - loss: 0.52290; acc: 0.752\n",
            "Test: epoch: 400 - loss: 0.55886; acc: 0.724\n",
            "Train: epoch: 500 - loss: 0.49801; acc: 0.764\n",
            "Test: epoch: 500 - loss: 0.54422; acc: 0.719\n",
            "Train: epoch: 600 - loss: 0.48241; acc: 0.771\n",
            "Test: epoch: 600 - loss: 0.53619; acc: 0.719\n",
            "Train: epoch: 700 - loss: 0.47259; acc: 0.777\n",
            "Test: epoch: 700 - loss: 0.53165; acc: 0.714\n",
            "Train: epoch: 800 - loss: 0.46626; acc: 0.773\n",
            "Test: epoch: 800 - loss: 0.52903; acc: 0.719\n",
            "Train: epoch: 900 - loss: 0.46208; acc: 0.775\n",
            "Test: epoch: 900 - loss: 0.52757; acc: 0.724\n",
            "Train: epoch: 1000 - loss: 0.45927; acc: 0.771\n",
            "Test: epoch: 1000 - loss: 0.52682; acc: 0.724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IhOoV327gWh"
      },
      "source": [
        "## Using PyTorch's DataLoader Class\r\n",
        "\r\n",
        "Sorry for the chunks of code above before starting the topic at hand. The above is so that we can compare the results with and without PyTorch's DataLoader class.\r\n",
        "\r\n",
        "Let's get right into it. We're going to import our required classes before moving forward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYfihJEtffuP"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o-P3Btk9cXi"
      },
      "source": [
        "You're going to see above that we imported the DataLoader class, but along with it, we also imported the Dataset class. This is because the DataLoader class accepts data in the form of a Dataset object.\r\n",
        "\r\n",
        "To get our data in the form of a Dataset object, we're going to create a custom class which is going to inherit from the Dataset class. Other than the making of an \\_\\_init\\_\\_ method, inheriting the Dataset class requires us to also override the \\_\\_getitem\\_\\_ and \\_\\_len\\_\\_ methods. The \\_\\_len\\_\\_ method returns the length of our Dataset object and the \\_\\_getitem\\_\\_ method returns the xy pair at a given index.\r\n",
        "\r\n",
        "Let's write the code for the class. \r\n",
        "\r\n",
        "Note: Something you'll notice is that I'm not preproceesing any of my data in the class, rather, I'm reusing all of my preprocessing from above. Due to the standardization and the splitting up of the data for train/test, I was unsure how to accomplish those steps in a clean way inside the class. Would love to get some advice in case I've been using the Dataset class the wrong way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOyi5nPGfyNN"
      },
      "source": [
        "class PimaIndiansDiabetes(Dataset):\n",
        "\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.len = len(self.X)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.X[index], self.y[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p1uOiKIAgt0"
      },
      "source": [
        "Due to the preprocessing being done above, the class is very straightforward. It's really just acting as a wrapper for our training and testing datasets, allowing them to be in a format acceptable for the DataLoader class.\r\n",
        "\r\n",
        "We'll continue by making Dataset objects for both the training and testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8HCmT0Vk_Fc"
      },
      "source": [
        "train_data = PimaIndiansDiabetes(X_train, y_train)\r\n",
        "test_data = PimaIndiansDiabetes(X_test, y_test)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbDWKqrvA-pj"
      },
      "source": [
        "Finally, time to use the DataLoader class.\r\n",
        "\r\n",
        "What we have below are the creation of 2 DataLoader objects - 1 for the training data and the other for the testing. The DataLoader class has many parameters which we can pass. Let's talk about the arguments we're passing it.\r\n",
        "\r\n",
        "__Batch_size__ - this 1 is self-explanatory. The DataLoader creates batches for us to be able to iterate through them. We no longer have to care about slicing the data to retrieve batches.\r\n",
        "\r\n",
        "__Shuffle__ - this allows our data to be shuffled, but more importantly, it shuffles our data every epoch. This trick allows our batches to be a random set of 64 records each time. This trick helps with generalization.\r\n",
        "\r\n",
        "__Drop_last__ - this is something I set as True only when I have shuffle set to True. This drops the last non-full batch. If you realized, our training set has a size of 514. When we divide that by 64, you'll see that our last batch only contains 1 item. That 1 item is an insufficient sample size to be able to fit the model.\r\n",
        "\r\n",
        "There's 1 more parameter which I haven't set but do wish to bring up. That parameter is num_workers. We're not making use of it, because we aren't using a GPU, but when you use many GPUs, num_workers allows us to take full advantage of all the GPUs. It allocates our data appropriately, allowing us to significantly improve the speed of our training process.\r\n",
        "\r\n",
        "To read more about the DataLoader class and its capabilities, I suggest you head on over to PyTorch's documentation and have a look at its capabilities: https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZL30F9wkoLA0"
      },
      "source": [
        "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True, drop_last=True)\r\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True, drop_last=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y66mBIQylBa2"
      },
      "source": [
        "Let's reset our model, along with the loss and optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kMSew7yrZP7"
      },
      "source": [
        "model = Model()\r\n",
        "BCE = nn.BCELoss()\r\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = lr)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6Rupa2LlJNQ"
      },
      "source": [
        "Let's run our model again, but this time, using the DataLoader class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8pHXrmNtJt2",
        "outputId": "c418f503-9119-4eb2-ecdf-ca2c18bb8abc"
      },
      "source": [
        "for epoch in range(epochs):\r\n",
        "    \r\n",
        "    iteration_loss = 0.\r\n",
        "    iteration_accuracy = 0.\r\n",
        "    \r\n",
        "    model.train()\r\n",
        "    for i, data in enumerate(train_loader):\r\n",
        "      X, y = data\r\n",
        "      y_pred = model(X.float())\r\n",
        "      loss = BCE(y_pred, y.reshape(-1,1).float())     \r\n",
        "      \r\n",
        "      iteration_loss += loss\r\n",
        "      iteration_accuracy += accuracy(y_pred, y)\r\n",
        "\r\n",
        "      optimizer.zero_grad()\r\n",
        "      loss.backward()\r\n",
        "      optimizer.step()\r\n",
        "\r\n",
        "    if(epoch % print_epoch == 0):\r\n",
        "        print('Train: epoch: {0} - loss: {1:.5f}; acc: {2:.3f}'.format(epoch, iteration_loss/(i+1), iteration_accuracy/(i+1)))    \r\n",
        "\r\n",
        "    iteration_loss = 0.\r\n",
        "    iteration_accuracy = 0.    \r\n",
        "\r\n",
        "    model.eval()\r\n",
        "    for i, data in enumerate(test_loader):\r\n",
        "      X, y = data\r\n",
        "      y_pred = model(X.float())\r\n",
        "      loss = BCE(y_pred, y.reshape(-1,1).float())\r\n",
        "\r\n",
        "      iteration_loss += loss\r\n",
        "      iteration_accuracy += accuracy(y_pred, y)\r\n",
        "\r\n",
        "    if(epoch % print_epoch == 0):\r\n",
        "        print('Test: epoch: {0} - loss: {1:.5f}; acc: {2:.3f}'.format(epoch, iteration_loss/(i+1), iteration_accuracy/(i+1)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: epoch: 0 - loss: 0.66246; acc: 0.646\n",
            "Test: epoch: 0 - loss: 0.62454; acc: 0.698\n",
            "Train: epoch: 100 - loss: 0.63621; acc: 0.646\n",
            "Test: epoch: 100 - loss: 0.62233; acc: 0.667\n",
            "Train: epoch: 200 - loss: 0.60738; acc: 0.646\n",
            "Test: epoch: 200 - loss: 0.59760; acc: 0.667\n",
            "Train: epoch: 300 - loss: 0.57139; acc: 0.676\n",
            "Test: epoch: 300 - loss: 0.58358; acc: 0.646\n",
            "Train: epoch: 400 - loss: 0.53193; acc: 0.740\n",
            "Test: epoch: 400 - loss: 0.52303; acc: 0.771\n",
            "Train: epoch: 500 - loss: 0.50465; acc: 0.773\n",
            "Test: epoch: 500 - loss: 0.51226; acc: 0.766\n",
            "Train: epoch: 600 - loss: 0.48522; acc: 0.779\n",
            "Test: epoch: 600 - loss: 0.49977; acc: 0.776\n",
            "Train: epoch: 700 - loss: 0.47455; acc: 0.775\n",
            "Test: epoch: 700 - loss: 0.51014; acc: 0.750\n",
            "Train: epoch: 800 - loss: 0.46725; acc: 0.781\n",
            "Test: epoch: 800 - loss: 0.51183; acc: 0.734\n",
            "Train: epoch: 900 - loss: 0.46253; acc: 0.783\n",
            "Test: epoch: 900 - loss: 0.51971; acc: 0.729\n",
            "Train: epoch: 1000 - loss: 0.45799; acc: 0.785\n",
            "Test: epoch: 1000 - loss: 0.51306; acc: 0.740\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IP7jDm6lbXf"
      },
      "source": [
        "Cool stuff! You can see that our code is much cleaner when using the DataLoader class, but also, our results are also slightly better. Both the accuracy and the loss for our model are slightly better when using the DataLoader class. It's the combination of these simple tricks which will take allow us to stand out in a crowd.\r\n",
        "\r\n",
        "That concludes our little bit with the DataLoader class. Hopefully, you're convinced as to why you need to add this to your arsenal when implementing neural networks."
      ]
    }
  ]
}